{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('3.9')"
  },
  "interpreter": {
   "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   age  affordibility  bought_insurance\n",
       "0   22              1                 0\n",
       "1   25              0                 0\n",
       "2   47              1                 1\n",
       "3   52              0                 0\n",
       "4   46              1                 1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>affordibility</th>\n      <th>bought_insurance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>47</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>52</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>46</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/parminder/Documents/MachineLearning/datasets/insurance_data.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Train and Test Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['age', 'affordibility']],df.bought_insurance, test_size=0.2, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing: Scale the data so that both age and affordibility are in same scaling range\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled['age'] = X_train_scaled['age']/100\n",
    "\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled['age'] = X_test_scaled['age']/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL BUILDING- 1) Using keras Tensor Flow\n",
    "#2) Using python scratch\n",
    "# Compare the results obtained in 1 and 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) First build a model in keras/tensorflow and see what weights and bias values it comes up with. We will than try to reproduce same weights and bias in our plain python implementation of gradient descent. Below is the architecture of our simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "=========] - 0s 7ms/step - loss: 0.4669 - accuracy: 0.9091\n",
      "Epoch 4804/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4669 - accuracy: 0.9091\n",
      "Epoch 4805/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4669 - accuracy: 0.9091\n",
      "Epoch 4806/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4668 - accuracy: 0.9091\n",
      "Epoch 4807/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4668 - accuracy: 0.9091\n",
      "Epoch 4808/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4668 - accuracy: 0.9091\n",
      "Epoch 4809/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4668 - accuracy: 0.9091\n",
      "Epoch 4810/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4668 - accuracy: 0.9091\n",
      "Epoch 4811/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4667 - accuracy: 0.9091\n",
      "Epoch 4812/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4667 - accuracy: 0.9091\n",
      "Epoch 4813/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4667 - accuracy: 0.9091\n",
      "Epoch 4814/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4667 - accuracy: 0.9091\n",
      "Epoch 4815/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4667 - accuracy: 0.9091\n",
      "Epoch 4816/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4666 - accuracy: 0.9091\n",
      "Epoch 4817/5000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4666 - accuracy: 0.9091\n",
      "Epoch 4818/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4666 - accuracy: 0.9091\n",
      "Epoch 4819/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4666 - accuracy: 0.9091\n",
      "Epoch 4820/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4666 - accuracy: 0.9091\n",
      "Epoch 4821/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4665 - accuracy: 0.9091\n",
      "Epoch 4822/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4665 - accuracy: 0.9091\n",
      "Epoch 4823/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4665 - accuracy: 0.9091\n",
      "Epoch 4824/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4665 - accuracy: 0.9091\n",
      "Epoch 4825/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4665 - accuracy: 0.9091\n",
      "Epoch 4826/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4664 - accuracy: 0.9091\n",
      "Epoch 4827/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4664 - accuracy: 0.9091\n",
      "Epoch 4828/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4664 - accuracy: 0.9091\n",
      "Epoch 4829/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4664 - accuracy: 0.9091\n",
      "Epoch 4830/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4664 - accuracy: 0.9091\n",
      "Epoch 4831/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4663 - accuracy: 0.9091\n",
      "Epoch 4832/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4663 - accuracy: 0.9091\n",
      "Epoch 4833/5000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.4663 - accuracy: 0.9091\n",
      "Epoch 4834/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4663 - accuracy: 0.9091\n",
      "Epoch 4835/5000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4663 - accuracy: 0.9091\n",
      "Epoch 4836/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4662 - accuracy: 0.9091\n",
      "Epoch 4837/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4662 - accuracy: 0.9091\n",
      "Epoch 4838/5000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4662 - accuracy: 0.9091\n",
      "Epoch 4839/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4662 - accuracy: 0.9091\n",
      "Epoch 4840/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4662 - accuracy: 0.9091\n",
      "Epoch 4841/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4661 - accuracy: 0.9091\n",
      "Epoch 4842/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4661 - accuracy: 0.9091\n",
      "Epoch 4843/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4661 - accuracy: 0.9091\n",
      "Epoch 4844/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4661 - accuracy: 0.9091\n",
      "Epoch 4845/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4661 - accuracy: 0.9091\n",
      "Epoch 4846/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4660 - accuracy: 0.9091\n",
      "Epoch 4847/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4660 - accuracy: 0.9091\n",
      "Epoch 4848/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4660 - accuracy: 0.9091\n",
      "Epoch 4849/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4660 - accuracy: 0.9091\n",
      "Epoch 4850/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4660 - accuracy: 0.9091\n",
      "Epoch 4851/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4659 - accuracy: 0.9091\n",
      "Epoch 4852/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4659 - accuracy: 0.9091\n",
      "Epoch 4853/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4659 - accuracy: 0.9091\n",
      "Epoch 4854/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4659 - accuracy: 0.9091\n",
      "Epoch 4855/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4659 - accuracy: 0.9091\n",
      "Epoch 4856/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4659 - accuracy: 0.9091\n",
      "Epoch 4857/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4658 - accuracy: 0.9091\n",
      "Epoch 4858/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4658 - accuracy: 0.9091\n",
      "Epoch 4859/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4658 - accuracy: 0.9091\n",
      "Epoch 4860/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4658 - accuracy: 0.9091\n",
      "Epoch 4861/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4658 - accuracy: 0.9091\n",
      "Epoch 4862/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4657 - accuracy: 0.9091\n",
      "Epoch 4863/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4657 - accuracy: 0.9091\n",
      "Epoch 4864/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4657 - accuracy: 0.9091\n",
      "Epoch 4865/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4657 - accuracy: 0.9091\n",
      "Epoch 4866/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4657 - accuracy: 0.9091\n",
      "Epoch 4867/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4656 - accuracy: 0.9091\n",
      "Epoch 4868/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4656 - accuracy: 0.9091\n",
      "Epoch 4869/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4656 - accuracy: 0.9091\n",
      "Epoch 4870/5000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4656 - accuracy: 0.9091\n",
      "Epoch 4871/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4656 - accuracy: 0.9091\n",
      "Epoch 4872/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4655 - accuracy: 0.9091\n",
      "Epoch 4873/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4655 - accuracy: 0.9091\n",
      "Epoch 4874/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4655 - accuracy: 0.9091\n",
      "Epoch 4875/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4655 - accuracy: 0.9091\n",
      "Epoch 4876/5000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4655 - accuracy: 0.9091\n",
      "Epoch 4877/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4654 - accuracy: 0.9091\n",
      "Epoch 4878/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4654 - accuracy: 0.9091\n",
      "Epoch 4879/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4654 - accuracy: 0.9091\n",
      "Epoch 4880/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4654 - accuracy: 0.9091\n",
      "Epoch 4881/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4654 - accuracy: 0.9091\n",
      "Epoch 4882/5000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4653 - accuracy: 0.9091\n",
      "Epoch 4883/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4653 - accuracy: 0.9091\n",
      "Epoch 4884/5000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.4653 - accuracy: 0.9091\n",
      "Epoch 4885/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4653 - accuracy: 0.9091\n",
      "Epoch 4886/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4653 - accuracy: 0.9091\n",
      "Epoch 4887/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4652 - accuracy: 0.9091\n",
      "Epoch 4888/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4652 - accuracy: 0.9091\n",
      "Epoch 4889/5000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4652 - accuracy: 0.9091\n",
      "Epoch 4890/5000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4652 - accuracy: 0.9091\n",
      "Epoch 4891/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4652 - accuracy: 0.9091\n",
      "Epoch 4892/5000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4651 - accuracy: 0.9091\n",
      "Epoch 4893/5000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4651 - accuracy: 0.9091\n",
      "Epoch 4894/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4651 - accuracy: 0.9091\n",
      "Epoch 4895/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4651 - accuracy: 0.9091\n",
      "Epoch 4896/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4651 - accuracy: 0.9091\n",
      "Epoch 4897/5000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4650 - accuracy: 0.9091\n",
      "Epoch 4898/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4650 - accuracy: 0.9091\n",
      "Epoch 4899/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4650 - accuracy: 0.9091\n",
      "Epoch 4900/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4650 - accuracy: 0.9091\n",
      "Epoch 4901/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4650 - accuracy: 0.9091\n",
      "Epoch 4902/5000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4650 - accuracy: 0.9091\n",
      "Epoch 4903/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4649 - accuracy: 0.9091\n",
      "Epoch 4904/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4649 - accuracy: 0.9091\n",
      "Epoch 4905/5000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4649 - accuracy: 0.9091\n",
      "Epoch 4906/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4649 - accuracy: 0.9091\n",
      "Epoch 4907/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4649 - accuracy: 0.9091\n",
      "Epoch 4908/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4648 - accuracy: 0.9091\n",
      "Epoch 4909/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4648 - accuracy: 0.9091\n",
      "Epoch 4910/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4648 - accuracy: 0.9091\n",
      "Epoch 4911/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4648 - accuracy: 0.9091\n",
      "Epoch 4912/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4648 - accuracy: 0.9091\n",
      "Epoch 4913/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4647 - accuracy: 0.9091\n",
      "Epoch 4914/5000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4647 - accuracy: 0.9091\n",
      "Epoch 4915/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4647 - accuracy: 0.9091\n",
      "Epoch 4916/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4647 - accuracy: 0.9091\n",
      "Epoch 4917/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4647 - accuracy: 0.9091\n",
      "Epoch 4918/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4646 - accuracy: 0.9091\n",
      "Epoch 4919/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4646 - accuracy: 0.9091\n",
      "Epoch 4920/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4646 - accuracy: 0.9091\n",
      "Epoch 4921/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4646 - accuracy: 0.9091\n",
      "Epoch 4922/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4646 - accuracy: 0.9091\n",
      "Epoch 4923/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4645 - accuracy: 0.9091\n",
      "Epoch 4924/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4645 - accuracy: 0.9091\n",
      "Epoch 4925/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4645 - accuracy: 0.9091\n",
      "Epoch 4926/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4645 - accuracy: 0.9091\n",
      "Epoch 4927/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4645 - accuracy: 0.9091\n",
      "Epoch 4928/5000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.4644 - accuracy: 0.9091\n",
      "Epoch 4929/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4644 - accuracy: 0.9091\n",
      "Epoch 4930/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4644 - accuracy: 0.9091\n",
      "Epoch 4931/5000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4644 - accuracy: 0.9091\n",
      "Epoch 4932/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4644 - accuracy: 0.9091\n",
      "Epoch 4933/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4644 - accuracy: 0.9091\n",
      "Epoch 4934/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4643 - accuracy: 0.9091\n",
      "Epoch 4935/5000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4643 - accuracy: 0.9091\n",
      "Epoch 4936/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4643 - accuracy: 0.9091\n",
      "Epoch 4937/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4643 - accuracy: 0.9091\n",
      "Epoch 4938/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4643 - accuracy: 0.9091\n",
      "Epoch 4939/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4642 - accuracy: 0.9091\n",
      "Epoch 4940/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4642 - accuracy: 0.9091\n",
      "Epoch 4941/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4642 - accuracy: 0.9091\n",
      "Epoch 4942/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4642 - accuracy: 0.9091\n",
      "Epoch 4943/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4642 - accuracy: 0.9091\n",
      "Epoch 4944/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4641 - accuracy: 0.9091\n",
      "Epoch 4945/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4641 - accuracy: 0.9091\n",
      "Epoch 4946/5000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4641 - accuracy: 0.9091\n",
      "Epoch 4947/5000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4641 - accuracy: 0.9091\n",
      "Epoch 4948/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4641 - accuracy: 0.9091\n",
      "Epoch 4949/5000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4640 - accuracy: 0.9091\n",
      "Epoch 4950/5000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4640 - accuracy: 0.9091\n",
      "Epoch 4951/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4640 - accuracy: 0.9091\n",
      "Epoch 4952/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4640 - accuracy: 0.9091\n",
      "Epoch 4953/5000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4640 - accuracy: 0.9091\n",
      "Epoch 4954/5000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4639 - accuracy: 0.9091\n",
      "Epoch 4955/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4639 - accuracy: 0.9091\n",
      "Epoch 4956/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4639 - accuracy: 0.9091\n",
      "Epoch 4957/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4639 - accuracy: 0.9091\n",
      "Epoch 4958/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4639 - accuracy: 0.9091\n",
      "Epoch 4959/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4639 - accuracy: 0.9091\n",
      "Epoch 4960/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4638 - accuracy: 0.9091\n",
      "Epoch 4961/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4638 - accuracy: 0.9091\n",
      "Epoch 4962/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4638 - accuracy: 0.9091\n",
      "Epoch 4963/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4638 - accuracy: 0.9091\n",
      "Epoch 4964/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4638 - accuracy: 0.9091\n",
      "Epoch 4965/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4637 - accuracy: 0.9091\n",
      "Epoch 4966/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4637 - accuracy: 0.9091\n",
      "Epoch 4967/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4637 - accuracy: 0.9091\n",
      "Epoch 4968/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4637 - accuracy: 0.9091\n",
      "Epoch 4969/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4637 - accuracy: 0.9091\n",
      "Epoch 4970/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4636 - accuracy: 0.9091\n",
      "Epoch 4971/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4636 - accuracy: 0.9091\n",
      "Epoch 4972/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4636 - accuracy: 0.9091\n",
      "Epoch 4973/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4636 - accuracy: 0.9091\n",
      "Epoch 4974/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4636 - accuracy: 0.9091\n",
      "Epoch 4975/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4635 - accuracy: 0.9091\n",
      "Epoch 4976/5000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4635 - accuracy: 0.9091\n",
      "Epoch 4977/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4635 - accuracy: 0.9091\n",
      "Epoch 4978/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4635 - accuracy: 0.9091\n",
      "Epoch 4979/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4635 - accuracy: 0.9091\n",
      "Epoch 4980/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4635 - accuracy: 0.9091\n",
      "Epoch 4981/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4634 - accuracy: 0.9091\n",
      "Epoch 4982/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4634 - accuracy: 0.9091\n",
      "Epoch 4983/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4634 - accuracy: 0.9091\n",
      "Epoch 4984/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4634 - accuracy: 0.9091\n",
      "Epoch 4985/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4634 - accuracy: 0.9091\n",
      "Epoch 4986/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4633 - accuracy: 0.9091\n",
      "Epoch 4987/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4633 - accuracy: 0.9091\n",
      "Epoch 4988/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4633 - accuracy: 0.9091\n",
      "Epoch 4989/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4633 - accuracy: 0.9091\n",
      "Epoch 4990/5000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4633 - accuracy: 0.9091\n",
      "Epoch 4991/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4632 - accuracy: 0.9091\n",
      "Epoch 4992/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4632 - accuracy: 0.9091\n",
      "Epoch 4993/5000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4632 - accuracy: 0.9091\n",
      "Epoch 4994/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4632 - accuracy: 0.9091\n",
      "Epoch 4995/5000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4632 - accuracy: 0.9091\n",
      "Epoch 4996/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4631 - accuracy: 0.9091\n",
      "Epoch 4997/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4631 - accuracy: 0.9091\n",
      "Epoch 4998/5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4631 - accuracy: 0.9091\n",
      "Epoch 4999/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4631 - accuracy: 0.9091\n",
      "Epoch 5000/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4631 - accuracy: 0.9091\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc83522e400>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, input_shape=(2,), activation= 'sigmoid', kernel_initializer= 'ones', bias_initializer = 'zeros')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "loss='binary_crossentropy',\n",
    "metrics=['accuracy']) \n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/1 [==============================] - 0s 379ms/step - loss: 0.3550 - accuracy: 1.0000\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.35497748851776123, 1.0]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test) #gave 1 means 100 percent accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.7054848 ],\n",
       "       [0.35569546],\n",
       "       [0.16827846],\n",
       "       [0.47801173],\n",
       "       [0.7260697 ],\n",
       "       [0.8294984 ]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "model.predict(X_test_scaled) #>0.5 - will buy insurance, if < 0.5- person wont buy insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2     1\n",
       "10    0\n",
       "21    0\n",
       "11    0\n",
       "14    1\n",
       "9     1\n",
       "Name: bought_insurance, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get the value of weights and bias from the model: coef , intercept is known w1, w2, bias\n",
    "coef , intercept = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[5.0608673],\n",
       "        [1.4086503]], dtype=float32),\n",
       " array([-2.913703], dtype=float32))"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "coef, intercept\n",
    "\n",
    "#this means that w1 = 5.060867 , w2 = 1.4086502, bias = 2.9137027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now without using tensor , doing the above with plain python, \n",
    "# Step 1: First define sigmoid function \n",
    "# y= 1 / (1+ e (power -x)\n",
    "#then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9999999847700205"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    import math \n",
    "    return 1 / (1+math.exp(-x))\n",
    "sigmoid(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    age  affordibility\n",
       "2    47              1\n",
       "10   18              1\n",
       "21   26              0\n",
       "11   28              1\n",
       "14   49              1\n",
       "9    61              1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>affordibility</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>47</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>18</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>26</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>28</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>49</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>61</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Form Prediction function with below formula\n",
    "#Instead of model.predict, write our own prediction function that uses w1,w2 and bias\n",
    "#w1*age + w2*affordibility + bias\n",
    "#w1= coef[0] , w2 = coef[1], bias = intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.705484819775958"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "def prediction_function(age, affordibility):\n",
    "    weighted_sum = coef[0] * age + coef[1] * affordibility + intercept\n",
    "    return sigmoid(weighted_sum)\n",
    "\n",
    "prediction_function(.47, 1) #this is same as model.predict(X_test_scaled) \n",
    "#this shows how prediction function workd once we have weights & bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.35569544317951163"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "prediction_function(.18, 1) #this is same as model.predict(X_test_scaled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after the above prediction using Sigmoid. Now we do it using Gradient Descent\n",
    "\n",
    "#Now we start implementing gradient descent in plain python. Again the goal is to come up with same w1, w2 and bias that keras model calculated. We want to show how keras/tensorflow would have computed these values internally using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Abs error or mae or called as COST FUNCTION = 1/n Sum [abs(yi-y^)]\n",
    "#indiviual errors is called LOSS i.e Total Error = err1+err2+...+err13 (since we have 13 training samples that was sent to neuron)\n",
    "#epoch means Going thru all training samples ONCE.\n",
    "#For MAE use loss = 'mean_absolute_error' \n",
    "#For Mean Squared Error MSE = 1/n Sum [abs(yi-y^)power2]. use loss = 'mean_absolute_error' \n",
    "\n",
    "#For Log Loss or Binary Cross Entropy = -1/n Sum [yi log(y^)+ (1-yi). log(1-y^)] loss = 'binary_cross_entropy'\n",
    "\n",
    "#PS: For Logistic Regression MSE should not be used.   Log Loss or Binary Cross Entropy is used. https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c \n",
    "\n",
    "\n",
    "def log_loss(y_true, y_predicted):\n",
    "    epsilon = 1e-15\n",
    "    y_predicted_new = [max(i, epsilon) for i in y_predicted]\n",
    "    y_predicted_new = [min(i, 1-epsilon) for i in y_predicted_new]\n",
    "    y_predicted_new = np.array(y_predicted_new)\n",
    "    return -np.mean(y_true * np.log(y_predicted_new) + (1-y_true)* np.log(1-y_predicted_new))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.99999386, 0.5       , 0.73105858])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "def sigmoid_numpy(X):\n",
    "    return 1/(1+np.exp(-X))\n",
    "\n",
    "sigmoid_numpy(np.array([12, 0 , 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient_descent func helps to find out weights i.e w1, w2, bias \n",
    "\n",
    "# w1 = w1 - learning rate *der/der W1          der/der W1  = 1/n SUM[xi (y^-yi)].      here mean is done using dot product and transpose\n",
    "# b = b - learning rate * der/der B             der/der b = 1/n SUM[(yi^ - yi)]\n",
    "def gradient_descent(age, affordibility, y_true, epochs, loss_threshold):\n",
    "    w1 = w2 = 1\n",
    "    bias = 0\n",
    "    rate = 0.5 \n",
    "    n = len(age)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        weighted_sum =  w1 * age + w2 * affordibility + bias \n",
    "        y_predicted = sigmoid_numpy(weighted_sum)\n",
    "        \n",
    "        loss = log_loss(y_true, y_predicted)\n",
    "\n",
    "        w1d= (1/n) * np.dot(np.transpose(age) , (y_predicted - y_true))\n",
    "\n",
    "        w2d = (1/n) * np.dot(np.transpose(affordibility) , (y_predicted - y_true))\n",
    "        bias_d = np.mean(y_predicted - y_true)\n",
    "\n",
    "        w1 = w1 - rate * w1d\n",
    "        w2 = w2 - rate * w2d\n",
    "        bias = bias - rate * bias_d\n",
    "\n",
    "        print(f'Epoch:{i}, w1: {w1}, w2: {w2}, bias:{bias}, loss:{loss}')\n",
    "\n",
    "        if(loss<=loss_threshold):\n",
    "            break\n",
    "        \n",
    "    return w1, w2, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "37484761812156, bias:-2.158892638895851, loss:0.5065955884569808\n",
      "Epoch:182, w1: 3.235768256672519, w2: 1.3943636051358148, bias:-2.1641946744139777, loss:0.5062627148655543\n",
      "Epoch:183, w1: 3.2474690264778157, w2: 1.394970782361845, bias:-2.1694814133009186, loss:0.505931227333758\n",
      "Epoch:184, w1: 3.2591476781228836, w2: 1.3955701697511878, bias:-2.1747529887145927, loss:0.5056011184990193\n",
      "Epoch:185, w1: 3.270804232829102, w2: 1.3961619261439753, bias:-2.1800095317363692, loss:0.505272381068909\n",
      "Epoch:186, w1: 3.2824387123797556, w2: 1.3967462073858943, bias:-2.185251171408572, loss:0.5049450078195657\n",
      "Epoch:187, w1: 3.294051139107937, w2: 1.3973231663843697, bias:-2.1904780347712607, loss:0.5046189915941715\n",
      "Epoch:188, w1: 3.3056415358846545, w2: 1.3978929531636493, bias:-2.1956902468983084, loss:0.5042943253014793\n",
      "Epoch:189, w1: 3.317209926107157, w2: 1.3984557149188142, bias:-2.2008879309327845, loss:0.5039710019143897\n",
      "Epoch:190, w1: 3.3287563336874557, w2: 1.3990115960687382, bias:-2.2060712081216627, loss:0.5036490144685731\n",
      "Epoch:191, w1: 3.340280783041054, w2: 1.3995607383080155, bias:-2.211240197849864, loss:0.5033283560611389\n",
      "Epoch:192, w1: 3.351783299075871, w2: 1.400103280657881, bias:-2.21639501767365, loss:0.5030090198493474\n",
      "Epoch:193, w1: 3.3632639071813593, w2: 1.4006393595161428, bias:-2.221535783353378, loss:0.502690999049364\n",
      "Epoch:194, w1: 3.3747226332178184, w2: 1.401169108706148, bias:-2.2266626088856363, loss:0.502374286935055\n",
      "Epoch:195, w1: 3.3861595035058896, w2: 1.4016926595248012, bias:-2.231775606534763, loss:0.5020588768368214\n",
      "Epoch:196, w1: 3.3975745448162384, w2: 1.4022101407896574, bias:-2.236874886863771, loss:0.5017447621404708\n",
      "Epoch:197, w1: 3.408967784359416, w2: 1.4027216788851056, bias:-2.2419605587646827, loss:0.5014319362861261\n",
      "Epoch:198, w1: 3.4203392497759015, w2: 1.4032273978076624, bias:-2.247032729488292, loss:0.5011203927671694\n",
      "Epoch:199, w1: 3.4316889691263146, w2: 1.4037274192103975, bias:-2.252091504673362, loss:0.5008101251292182\n",
      "Epoch:200, w1: 3.443016970881803, w2: 1.4042218624465033, bias:-2.2571369883752723, loss:0.5005011269691375\n",
      "Epoch:201, w1: 3.4543232839145968, w2: 1.4047108446120309, bias:-2.2621692830941247, loss:0.5001933919340792\n",
      "Epoch:202, w1: 3.4656079374887283, w2: 1.4051944805878065, bias:-2.2671884898023182, loss:0.4998869137205552\n",
      "Epoch:203, w1: 3.4768709612509157, w2: 1.4056728830805474, bias:-2.2721947079716065, loss:0.499581686073539\n",
      "Epoch:204, w1: 3.4881123852216054, w2: 1.4061461626631908, bias:-2.2771880355996448, loss:0.4992777027855931\n",
      "Epoch:205, w1: 3.4993322397861726, w2: 1.406614427814455, bias:-2.282168569236039, loss:0.4989749576960279\n",
      "Epoch:206, w1: 3.510530555686274, w2: 1.407077784957646, bias:-2.287136404007907, loss:0.4986734446900829\n",
      "Epoch:207, w1: 3.5217073640113545, w2: 1.407536338498725, bias:-2.2920916336449575, loss:0.49837315769813556\n",
      "Epoch:208, w1: 3.5328626961903016, w2: 1.4079901908636536, bias:-2.2970343505041027, loss:0.49807409069493364\n",
      "Epoch:209, w1: 3.5439965839832466, w2: 1.4084394425350295, bias:-2.30196464559361, loss:0.49777623769885115\n",
      "Epoch:210, w1: 3.5551090594735104, w2: 1.4088841920880264, bias:-2.3068826085968004, loss:0.49747959277116743\n",
      "Epoch:211, w1: 3.5662001550596885, w2: 1.4093245362256548, bias:-2.3117883278953073, loss:0.4971841500153676\n",
      "Epoch:212, w1: 3.577269903447877, w2: 1.4097605698133546, bias:-2.3166818905919, loss:0.4968899035764641\n",
      "Epoch:213, w1: 3.5883183376440364, w2: 1.4101923859129344, bias:-2.321563382532884, loss:0.49659684764033973\n",
      "Epoch:214, w1: 3.5993454909464844, w2: 1.4106200758158696, bias:-2.32643288833008, loss:0.49630497643310856\n",
      "Epoch:215, w1: 3.610351396938527, w2: 1.4110437290759732, bias:-2.331290491382403, loss:0.49601428422049737\n",
      "Epoch:216, w1: 3.6213360894812157, w2: 1.4114634335414504, bias:-2.3361362738970306, loss:0.4957247653072439\n",
      "Epoch:217, w1: 3.6322996027062318, w2: 1.4118792753863514, bias:-2.3409703169101865, loss:0.49543641403651506\n",
      "Epoch:218, w1: 3.6432419710088983, w2: 1.4122913391414305, bias:-2.345792700307536, loss:0.49514922478933926\n",
      "Epoch:219, w1: 3.6541632290413135, w2: 1.4126997077244283, bias:-2.350603502844203, loss:0.4948631919840572\n",
      "Epoch:220, w1: 3.6650634117056042, w2: 1.4131044624697842, bias:-2.3554028021644196, loss:0.49457831007578845\n",
      "Epoch:221, w1: 3.6759425541473023, w2: 1.4135056831577921, bias:-2.360190674820809, loss:0.4942945735559119\n",
      "Epoch:222, w1: 3.6868006917488327, w2: 1.4139034480432113, bias:-2.3649671962933168, loss:0.49401197695156235\n",
      "Epoch:223, w1: 3.6976378601231215, w2: 1.4142978338833405, bias:-2.3697324410077902, loss:0.4937305148251413\n",
      "Epoch:224, w1: 3.7084540951073124, w2: 1.4146889159655687, bias:-2.3744864823542158, loss:0.49345018177384004\n",
      "Epoch:225, w1: 3.719249432756598, w2: 1.4150767681344103, bias:-2.379229392704622, loss:0.49317097242917785\n",
      "Epoch:226, w1: 3.7300239093381586, w2: 1.4154614628180373, bias:-2.383961243430653, loss:0.4928928814565522\n",
      "Epoch:227, w1: 3.740777561325208, w2: 1.4158430710543157, bias:-2.388682104920817, loss:0.4926159035548014\n",
      "Epoch:228, w1: 3.751510425391147, w2: 1.4162216625163582, bias:-2.393392046597421, loss:0.4923400334557782\n",
      "Epoch:229, w1: 3.7622225384038157, w2: 1.4165973055376, bias:-2.398091136933194, loss:0.4920652659239369\n",
      "Epoch:230, w1: 3.772913937419856, w2: 1.4169700671364103, bias:-2.4027794434676064, loss:0.4917915957559304\n",
      "Epoch:231, w1: 3.783584659679165, w2: 1.417340013040245, bias:-2.407457032822888, loss:0.49151901778021734\n",
      "Epoch:232, w1: 3.7942347425994543, w2: 1.4177072077093518, bias:-2.412123970719756, loss:0.4912475268566808\n",
      "Epoch:233, w1: 3.804864223770903, w2: 1.4180717143600363, bias:-2.4167803219928548, loss:0.4909771178762567\n",
      "Epoch:234, w1: 3.8154731409509055, w2: 1.418433594987496, bias:-2.4214261506059125, loss:0.4907077857605718\n",
      "Epoch:235, w1: 3.8260615320589157, w2: 1.4187929103882317, bias:-2.426061519666623, loss:0.49043952546159064\n",
      "Epoch:236, w1: 3.83662943517138, w2: 1.4191497201820455, bias:-2.4306864914412545, loss:0.4901723319612717\n",
      "Epoch:237, w1: 3.8471768885167634, w2: 1.41950408283363, bias:-2.4353011273689953, loss:0.4899062002712329\n",
      "Epoch:238, w1: 3.857703930470663, w2: 1.4198560556737607, bias:-2.4399054880760342, loss:0.48964112543242444\n",
      "Epoch:239, w1: 3.8682105995510105, w2: 1.4202056949200956, bias:-2.444499633389389, loss:0.48937710251481087\n",
      "Epoch:240, w1: 3.8786969344133597, w2: 1.4205530556975927, bias:-2.4490836223504795, loss:0.48911412661705983\n",
      "Epoch:241, w1: 3.8891629738462594, w2: 1.4208981920585493, bias:-2.45365751322846, loss:0.48885219286623943\n",
      "Epoch:242, w1: 3.8996087567667086, w2: 1.4212411570022738, bias:-2.4582213635333026, loss:0.48859129641752214\n",
      "Epoch:243, w1: 3.910034322215694, w2: 1.4215820024943946, bias:-2.4627752300286496, loss:0.48833143245389626\n",
      "Epoch:244, w1: 3.920439709353808, w2: 1.4219207794858135, bias:-2.4673191687444302, loss:0.48807259618588394\n",
      "Epoch:245, w1: 3.930824957456947, w2: 1.4222575379313118, bias:-2.471853234989249, loss:0.48781478285126634\n",
      "Epoch:246, w1: 3.9411901059120837, w2: 1.422592326807813, bias:-2.4763774833625503, loss:0.4875579877148143\n",
      "Epoch:247, w1: 3.9515351942131214, w2: 1.422925194132311, bias:-2.4808919677665644, loss:0.4873022060680255\n",
      "Epoch:248, w1: 3.9618602619568177, w2: 1.4232561869794709, bias:-2.485396741418036, loss:0.487047433228868\n",
      "Epoch:249, w1: 3.972165348838788, w2: 1.4235853514989025, bias:-2.489891856859742, loss:0.4867936645415293\n",
      "Epoch:250, w1: 3.982450494649576, w2: 1.4239127329321233, bias:-2.494377365971801, loss:0.48654089537617085\n",
      "Epoch:251, w1: 3.9927157392707997, w2: 1.4242383756292052, bias:-2.49885331998278, loss:0.48628912112868766\n",
      "Epoch:252, w1: 4.002961122671366, w2: 1.42456232306512, bias:-2.503319769480601, loss:0.48603833722047357\n",
      "Epoch:253, w1: 4.013186684903756, w2: 1.4248846178557855, bias:-2.50777676442325, loss:0.48578853909819186\n",
      "Epoch:254, w1: 4.023392466100375, w2: 1.4252053017738153, bias:-2.512224354149295, loss:0.4855397222335499\n",
      "Epoch:255, w1: 4.033578506469971, w2: 1.425524415763985, bias:-2.516662587388215, loss:0.4852918821230787\n",
      "Epoch:256, w1: 4.043744846294124, w2: 1.4258419999584127, bias:-2.5210915122705426, loss:0.4850450142879177\n",
      "Epoch:257, w1: 4.053891525923789, w2: 1.4261580936914633, bias:-2.525511176337824, loss:0.48479911427360317\n",
      "Epoch:258, w1: 4.064018585775913, w2: 1.4264727355143816, bias:-2.529921626552404, loss:0.48455417764986114\n",
      "Epoch:259, w1: 4.074126066330109, w2: 1.4267859632096573, bias:-2.534322909307031, loss:0.48431020001040465\n",
      "Epoch:260, w1: 4.084214008125393, w2: 1.4270978138051291, bias:-2.5387150704342933, loss:0.48406717697273477\n",
      "Epoch:261, w1: 4.094282451756979, w2: 1.4274083235878308, bias:-2.5430981552158864, loss:0.4838251041779449\n",
      "Epoch:262, w1: 4.104331437873139, w2: 1.4277175281175865, bias:-2.5474722083917123, loss:0.48358397729053065\n",
      "Epoch:263, w1: 4.114361007172114, w2: 1.428025462240357, bias:-2.5518372741688182, loss:0.4833437919982009\n",
      "Epoch:264, w1: 4.124371200399091, w2: 1.4283321601013441, bias:-2.5561933962301766, loss:0.4831045440116944\n",
      "Epoch:265, w1: 4.134362058343228, w2: 1.4286376551578572, bias:-2.5605406177433045, loss:0.48286622906459864\n",
      "Epoch:266, w1: 4.144333621834739, w2: 1.4289419801919439, bias:-2.564878981368735, loss:0.48262884291317154\n",
      "Epoch:267, w1: 4.1542859317420335, w2: 1.429245167322793, bias:-2.569208529268332, loss:0.48239238133616846\n",
      "Epoch:268, w1: 4.164219028968911, w2: 1.4295472480189122, bias:-2.573529303113461, loss:0.48215684013466936\n",
      "Epoch:269, w1: 4.174132954451802, w2: 1.4298482531100827, bias:-2.577841344093013, loss:0.481922215131912\n",
      "Epoch:270, w1: 4.184027749157071, w2: 1.430148212799099, bias:-2.5821446929212852, loss:0.4816885021731251\n",
      "Epoch:271, w1: 4.1939034540783595, w2: 1.4304471566732948, bias:-2.5864393898457227, loss:0.4814556971253665\n",
      "Epoch:272, w1: 4.203760110233989, w2: 1.4307451137158596, bias:-2.5907254746545223, loss:0.4812237958773629\n",
      "Epoch:273, w1: 4.213597758664405, w2: 1.4310421123169506, bias:-2.5950029866841016, loss:0.480992794339352\n",
      "Epoch:274, w1: 4.223416440429678, w2: 1.4313381802846028, bias:-2.5992719648264337, loss:0.48076268844292946\n",
      "Epoch:275, w1: 4.233216196607044, w2: 1.4316333448554421, bias:-2.6035324475362565, loss:0.48053347414089403\n",
      "Epoch:276, w1: 4.242997068288498, w2: 1.4319276327052033, bias:-2.607784472838149, loss:0.48030514740709995\n",
      "Epoch:277, w1: 4.252759096578431, w2: 1.4322210699590585, bias:-2.612028078333486, loss:0.4800777042363081\n",
      "Epoch:278, w1: 4.262502322591309, w2: 1.4325136822017583, bias:-2.616263301207267, loss:0.4798511406440404\n",
      "Epoch:279, w1: 4.272226787449408, w2: 1.4328054944875899, bias:-2.620490178234828, loss:0.47962545266643786\n",
      "Epoch:280, w1: 4.281932532280578, w2: 1.433096531350154, bias:-2.6247087457884315, loss:0.4794006363601184\n",
      "Epoch:281, w1: 4.291619598216061, w2: 1.4333868168119661, bias:-2.628919039843741, loss:0.4791766878020384\n",
      "Epoch:282, w1: 4.301288026388348, w2: 1.4336763743938834, bias:-2.633121095986182, loss:0.4789536030893553\n",
      "Epoch:283, w1: 4.3109378579290745, w2: 1.4339652271243617, bias:-2.637314949417191, loss:0.478731378339293\n",
      "Epoch:284, w1: 4.320569133966964, w2: 1.4342533975485452, bias:-2.6415006349603525, loss:0.47851000968900853\n",
      "Epoch:285, w1: 4.330181895625803, w2: 1.4345409077371913, bias:-2.6456781870674306, loss:0.4782894932954603\n",
      "Epoch:286, w1: 4.339776184022467, w2: 1.4348277792954356, bias:-2.6498476398242916, loss:0.47806982533527886\n",
      "Epoch:287, w1: 4.349352040264972, w2: 1.435114033371397, bias:-2.6540090269567256, loss:0.47785100200463954\n",
      "Epoch:288, w1: 4.358909505450577, w2: 1.4353996906646287, bias:-2.658162381836163, loss:0.4776330195191349\n",
      "Epoch:289, w1: 4.368448620663914, w2: 1.4356847714344168, bias:-2.6623077374852917, loss:0.47741587411365194\n",
      "Epoch:290, w1: 4.377969426975163, w2: 1.4359692955079275, bias:-2.666445126583577, loss:0.4771995620422475\n",
      "Epoch:291, w1: 4.387471965438257, w2: 1.43625328228821, bias:-2.6705745814726813, loss:0.47698407957802763\n",
      "Epoch:292, w1: 4.396956277089129, w2: 1.436536750762052, bias:-2.674696134161793, loss:0.476769423013028\n",
      "Epoch:293, w1: 4.406422402943988, w2: 1.4368197195076962, bias:-2.6788098163328593, loss:0.47655558865809444\n",
      "Epoch:294, w1: 4.415870383997634, w2: 1.4371022067024166, bias:-2.682915659345727, loss:0.4763425728427668\n",
      "Epoch:295, w1: 4.425300261221806, w2: 1.437384230129958, bias:-2.6870136942431952, loss:0.476130371915163\n",
      "Epoch:296, w1: 4.434712075563564, w2: 1.4376658071878416, bias:-2.69110395175598, loss:0.47591898224186396\n",
      "Epoch:297, w1: 4.4441058679436996, w2: 1.4379469548945394, bias:-2.6951864623075887, loss:0.4757084002078021\n",
      "Epoch:298, w1: 4.453481679255187, w2: 1.4382276898965187, bias:-2.6992612560191143, loss:0.47549862221614797\n",
      "Epoch:299, w1: 4.462839550361659, w2: 1.4385080284751603, bias:-2.703328362713941, loss:0.4752896446882009\n",
      "Epoch:300, w1: 4.472179522095915, w2: 1.438787986553552, bias:-2.707387811922373, loss:0.4750814640632793\n",
      "Epoch:301, w1: 4.4815016352584625, w2: 1.439067579703159, bias:-2.711439632886177, loss:0.47487407679861204\n",
      "Epoch:302, w1: 4.49080593061609, w2: 1.4393468231503754, bias:-2.7154838545630504, loss:0.47466747936923254\n",
      "Epoch:303, w1: 4.500092448900464, w2: 1.4396257317829577, bias:-2.7195205056310083, loss:0.47446166826787173\n",
      "Epoch:304, w1: 4.5093612308067605, w2: 1.4399043201563404, bias:-2.723549614492697, loss:0.4742566400048545\n",
      "Epoch:305, w1: 4.518612316992322, w2: 1.440182602499841, bias:-2.72757120927963, loss:0.4740523911079949\n",
      "Epoch:306, w1: 4.527845748075346, w2: 1.4404605927227512, bias:-2.731585317856352, loss:0.4738489181224942\n",
      "Epoch:307, w1: 4.537061564633595, w2: 1.440738304420319, bias:-2.7355919678245306, loss:0.47364621761083875\n",
      "Epoch:308, w1: 4.546259807203141, w2: 1.441015750879624, bias:-2.7395911865269764, loss:0.47344428615270023\n",
      "Epoch:309, w1: 4.555440516277136, w2: 1.4412929450853456, bias:-2.7435830010515936, loss:0.4732431203448344\n",
      "Epoch:310, w1: 4.564603732304602, w2: 1.4415698997254285, bias:-2.747567438235262, loss:0.473042716800984\n",
      "Epoch:311, w1: 4.573749495689255, w2: 1.441846627196646, bias:-2.7515445246676515, loss:0.4728430721517801\n",
      "Epoch:312, w1: 4.58287784678835, w2: 1.4421231396100629, bias:-2.7555142866949733, loss:0.4726441830446452\n",
      "Epoch:313, w1: 4.591988825911552, w2: 1.4423994487964005, bias:-2.759476750423661, loss:0.4724460461436974\n",
      "Epoch:314, w1: 4.601082473319831, w2: 1.4426755663113053, bias:-2.763431941723993, loss:0.47224865812965516\n",
      "Epoch:315, w1: 4.610158829224385, w2: 1.4429515034405223, bias:-2.7673798862336514, loss:0.47205201569974303\n",
      "Epoch:316, w1: 4.619217933785582, w2: 1.4432272712049754, bias:-2.7713206093612155, loss:0.47185611556759816\n",
      "Epoch:317, w1: 4.628259827111926, w2: 1.4435028803657577, bias:-2.7752541362896017, loss:0.4716609544631776\n",
      "Epoch:318, w1: 4.63728454925905, w2: 1.44377834142903, bias:-2.7791804919794383, loss:0.47146652913266657\n",
      "Epoch:319, w1: 4.646292140228726, w2: 1.4440536646508326, bias:-2.7830997011723855, loss:0.4712728363383867\n",
      "Epoch:320, w1: 4.655282639967903, w2: 1.4443288600418103, bias:-2.787011788394397, loss:0.4710798728587066\n",
      "Epoch:321, w1: 4.664256088367763, w2: 1.4446039373718518, bias:-2.7909167779589263, loss:0.4708876354879519\n",
      "Epoch:322, w1: 4.673212525262796, w2: 1.4448789061746472, bias:-2.7948146939700766, loss:0.470696121036316\n",
      "Epoch:323, w1: 4.682151990429907, w2: 1.445153775752162, bias:-2.7987055603256983, loss:0.4705053263297723\n",
      "Epoch:324, w1: 4.691074523587528, w2: 1.445428555179031, bias:-2.8025894007204317, loss:0.4703152482099871\n",
      "Epoch:325, w1: 4.6999801643947645, w2: 1.4457032533068734, bias:-2.8064662386487, loss:0.47012588353423135\n",
      "Epoch:326, w1: 4.708868952450553, w2: 1.445977878768531, bias:-2.8103360974076472, loss:0.4699372291752969\n",
      "Epoch:327, w1: 4.7177409272928434, w2: 1.4462524399822285, bias:-2.81419900010003, loss:0.46974928202140903\n",
      "Epoch:328, w1: 4.726596128397797, w2: 1.4465269451556608, bias:-2.818054969637057, loss:0.4695620389761421\n",
      "Epoch:329, w1: 4.735434595179007, w2: 1.446801402290005, bias:-2.82190402874118, loss:0.4693754969583367\n",
      "Epoch:330, w1: 4.744256366986734, w2: 1.4470758191838622, bias:-2.82574619994884, loss:0.46918965290201436\n",
      "Epoch:331, w1: 4.753061483107164, w2: 1.4473502034371264, bias:-2.8295815056131626, loss:0.469004503756296\n",
      "Epoch:332, w1: 4.76184998276168, w2: 1.4476245624547852, bias:-2.833409967906611, loss:0.46882004648531833\n",
      "Epoch:333, w1: 4.770621905106156, w2: 1.4478989034506518, bias:-2.8372316088235907, loss:0.46863627806815394\n",
      "Epoch:334, w1: 4.779377289230264, w2: 1.448173233451029, bias:-2.841046450183012, loss:0.4684531954987285\n",
      "Epoch:335, w1: 4.7881161741568015, w2: 1.4484475592983093, bias:-2.8448545136308083, loss:0.46827079578574116\n",
      "Epoch:336, w1: 4.796838598841035, w2: 1.448721887654507, bias:-2.848655820642411, loss:0.46808907595258414\n",
      "Epoch:337, w1: 4.805544602170057, w2: 1.4489962250047304, bias:-2.852450392525183, loss:0.467908033037264\n",
      "Epoch:338, w1: 4.814234222962166, w2: 1.4492705776605888, bias:-2.8562382504208115, loss:0.4677276640923221\n",
      "Epoch:339, w1: 4.822907499966253, w2: 1.4495449517635393, bias:-2.860019415307658, loss:0.4675479661847561\n",
      "Epoch:340, w1: 4.831564471861215, w2: 1.4498193532881727, bias:-2.8637939080030708, loss:0.46736893639594346\n",
      "Epoch:341, w1: 4.840205177255373, w2: 1.4500937880454414, bias:-2.8675617491656573, loss:0.4671905718215625\n",
      "Epoch:342, w1: 4.848829654685914, w2: 1.4503682616858278, bias:-2.8713229592975185, loss:0.46701286957151705\n",
      "Epoch:343, w1: 4.85743794261834, w2: 1.4506427797024561, bias:-2.875077558746444, loss:0.46683582676985935\n",
      "Epoch:344, w1: 4.866030079445939, w2: 1.450917347434149, bias:-2.8788255677080725, loss:0.4666594405547149\n",
      "Epoch:345, w1: 4.874606103489267, w2: 1.4511919700684275, bias:-2.882567006228014, loss:0.46648370807820677\n",
      "Epoch:346, w1: 4.883166052995643, w2: 1.4514666526444582, bias:-2.8863018942039362, loss:0.4663086265063812\n",
      "Epoch:347, w1: 4.891709966138657, w2: 1.4517414000559465, bias:-2.8900302513876173, loss:0.466134193019133\n",
      "Epoch:348, w1: 4.900237881017698, w2: 1.4520162170539788, bias:-2.893752097386962, loss:0.4659604048101324\n",
      "Epoch:349, w1: 4.9087498356574875, w2: 1.4522911082498116, bias:-2.8974674516679864, loss:0.4657872590867513\n",
      "Epoch:350, w1: 4.917245868007634, w2: 1.4525660781176122, bias:-2.901176333556766, loss:0.46561475306999006\n",
      "Epoch:351, w1: 4.925726015942192, w2: 1.4528411309971487, bias:-2.9048787622413546, loss:0.4654428839944057\n",
      "Epoch:352, w1: 4.9341903172592385, w2: 1.453116271096432, bias:-2.9085747567736684, loss:0.4652716491080398\n",
      "Epoch:353, w1: 4.942638809680464, w2: 1.45339150249431, bias:-2.91226433607134, loss:0.46510104567234706\n",
      "Epoch:354, w1: 4.95107153085077, w2: 1.453666829143015, bias:-2.9159475189195407, loss:0.4649310709621242\n",
      "Epoch:355, w1: 4.959488518337886, w2: 1.453942254870665, bias:-2.919624323972772, loss:0.46476172226543927\n",
      "Epoch:356, w1: 4.967889809631988, w2: 1.4542177833837202, bias:-2.9232947697566276, loss:0.4645929968835612\n",
      "Epoch:357, w1: 4.976275442145338, w2: 1.4544934182693938, bias:-2.9269588746695274, loss:0.46442489213089116\n",
      "Epoch:358, w1: 4.984645453211936, w2: 1.4547691629980217, bias:-2.93061665698442, loss:0.464257405334892\n",
      "Epoch:359, w1: 4.9929998800871696, w2: 1.4550450209253871, bias:-2.934268134850457, loss:0.46409053383601956\n",
      "Epoch:360, w1: 5.001338759947489, w2: 1.4553209952950035, bias:-2.9379133262946424, loss:0.46392427498765465\n",
      "Epoch:361, w1: 5.00966212989009, w2: 1.4555970892403576, bias:-2.9415522492234514, loss:0.46375862615603475\n",
      "Epoch:362, w1: 5.0179700269326, w2: 1.45587330578711, bias:-2.9451849214244237, loss:0.46359358472018636\n",
      "Epoch:363, w1: 5.026262488012782, w2: 1.456149647855258, bias:-2.94881136056773, loss:0.4634291480718573\n",
      "Epoch:364, w1: 5.0345395499882475, w2: 1.456426118261257, bias:-2.952431584207713, loss:0.46326531361545026\n",
      "Epoch:365, w1: 5.042801249636176, w2: 1.4567027197201048, bias:-2.956045609784402, loss:0.46310207876795667\n",
      "Epoch:366, w1: 5.051047623653049, w2: 1.4569794548473887, bias:-2.9596534546250037, loss:0.46293944095888917\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5.051047623653049, 1.4569794548473887, -2.9596534546250037)"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "#how to call the gradient descent function? passing the parameter\n",
    "#age - its a vector AGE , not single age value : so we numpy \n",
    "# \n",
    "gradient_descent(X_train_scaled['age'], X_train_scaled['affordibility'], y_train, 5000, 0.4631)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[5.0608673],\n",
       "        [1.4086503]], dtype=float32),\n",
       " array([-2.913703], dtype=float32))"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "coef, intercept # and the results totally match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This shows that in the end we were able to come up with same value of w1,w2 and bias using a plain python implementation of gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing with the help of CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNN:\n",
    "    def __init__(self):\n",
    "        self.w1 = 1\n",
    "        self.w2 = 1\n",
    "        self.bias = 0\n",
    "\n",
    "    def fit(self, X, y, epochs, loss_threshold):\n",
    "        self.w1, self.w2, self.bias = self.gradient_descent(X['age'], X['affordibility'], y, epochs, loss_threshold)\n",
    "        print(f\"Final weights and bias: w1: {self.w1}, w2: {self.w2}, bias: {self.bias}\")\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        weighted_sum = self.w1*X_test['age'] + self.w2*X_test['affordibility'] + self.bias\n",
    "        return sigmoid_numpy(weighted_sum)\n",
    "\n",
    "    def gradient_descent(self, age,affordability, y_true, epochs, loss_thresold):\n",
    "        w1 = w2 = 1\n",
    "        bias = 0\n",
    "        rate = 0.5\n",
    "        n = len(age)\n",
    "        for i in range(epochs):\n",
    "            weighted_sum = w1 * age + w2 * affordability + bias\n",
    "            y_predicted = sigmoid_numpy(weighted_sum)\n",
    "            loss = log_loss(y_true, y_predicted)\n",
    "            \n",
    "            w1d = (1/n)*np.dot(np.transpose(age),(y_predicted-y_true)) \n",
    "            w2d = (1/n)*np.dot(np.transpose(affordability),(y_predicted-y_true)) \n",
    "\n",
    "            bias_d = np.mean(y_predicted-y_true)\n",
    "            w1 = w1 - rate * w1d\n",
    "            w2 = w2 - rate * w2d\n",
    "            bias = bias - rate * bias_d\n",
    "            \n",
    "            if i%50==0:\n",
    "                print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')\n",
    "            \n",
    "            if loss<=loss_thresold:\n",
    "                print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')\n",
    "                break\n",
    "\n",
    "        return w1, w2, bias  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:0, w1:0.974907633470177, w2:0.948348125394529, bias:-0.11341867736368583, loss:0.7113403233723417\n",
      "Epoch:50, w1:1.503319554173139, w2:1.108384790367645, bias:-1.2319047301235464, loss:0.5675865113475955\n",
      "Final weights and bias: w1: 2.1871944491052364, w2: 1.2918774898345378, bias: -1.6533819823304428\n"
     ]
    }
   ],
   "source": [
    "customModel = myNN()\n",
    "customModel.fit(X_train_scaled, y_train, epochs= 100, loss_threshold=0.4631)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[5.0608673],\n",
       "        [1.4086503]], dtype=float32),\n",
       " array([-2.913703], dtype=float32))"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "coef, intercept\n",
    "#This shows that in the end we were able to come up with same value of w1,w2 and bias using a plain python implementation of gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     age  affordibility\n",
       "2   0.47              1\n",
       "10  0.18              1\n",
       "21  0.26              0\n",
       "11  0.28              1\n",
       "14  0.49              1\n",
       "9   0.61              1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>affordibility</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>0.47</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.18</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.26</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.28</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.49</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.61</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2     0.695713\n",
       "10    0.636006\n",
       "21    0.451655\n",
       "11    0.657189\n",
       "14    0.699625\n",
       "9     0.722476\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "#1) Predict using custom model\n",
    "customModel.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.7054848 ],\n",
       "       [0.35569546],\n",
       "       [0.16827846],\n",
       "       [0.47801173],\n",
       "       [0.7260697 ],\n",
       "       [0.8294984 ]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "#(2) Predict using tensorflow model\n",
    "model.predict(X_test_scaled)\n",
    "#Above you can compare predictions from our own custom model and tensoflow model. You will notice that predictions are almost same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/codebasics/deep-learning-keras-tf-tutorial/blob/master/7_nn_from_scratch/7_neural_network_from_scratch.ipynb"
   ]
  }
 ]
}